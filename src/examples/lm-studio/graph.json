{
	"edges": [
		{
			"from": "main",
			"to": "output-2",
			"out": "output",
			"in": "output"
		},
		{
			"from": "fetch-4",
			"to": "main",
			"out": "*",
			"in": ""
		},
		{
			"from": "fn-3",
			"to": "fetch-4",
			"out": "payload",
			"in": "body"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "systemContext",
			"in": "systemContext"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "userContext",
			"in": "userContext"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "systemRole",
			"in": "systemRole"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "userRole",
			"in": "userRole"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "temperature",
			"in": "temperature"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "max_tokens",
			"in": "max_tokens"
		},
		{
			"from": "query",
			"to": "fn-3",
			"out": "stream",
			"in": "steam"
		}
	],
	"nodes": [
		{
			"id": "output-2",
			"type": "output",
			"configuration": {
				"schema": {
					"type": "object",
					"properties": {
						"output": {
							"type": "string",
							"title": "output"
						}
					}
				}
			}
		},
		{
			"id": "main",
			"type": "output",
			"configuration": {}
		},
		{
			"id": "fetch-4",
			"type": "fetch",
			"configuration": {
				"headers": {
					"content-type": "application/json"
				},
				"method": "POST",
				"url": "http://host.docker.internal:1234/v1/chat/completions"
			}
		},
		{
			"id": "fn-3",
			"type": "invoke",
			"configuration": {
				"$board": "#fn-3"
			}
		},
		{
			"id": "query",
			"type": "input",
			"configuration": {
				"schema": {
					"title": "LM Studio Schema for https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
					"properties": {
						"systemContext": {
							"type": "string",
							"title": "system context",
							"default": "default",
							"description": "context of the system"
						},
						"systemRole": {
							"type": "string",
							"title": "system role",
							"default": "system",
							"description": "role of the system"
						},
						"userContext": {
							"type": "string",
							"title": "user context",
							"default": "default",
							"description": "context of the user"
						},
						"userRole": {
							"type": "string",
							"title": "user context",
							"default": "user",
							"description": "role of the user"
						},
						"temperature": {
							"type": "number",
							"title": "temperature",
							"default": "1.0",
							"description": "The temperature of the sampling operation. 1 means regular sampling, 0 means always take the highest score, 100.0 is getting closer to uniform probability"
						},
						"maxToken": {
							"type": "number",
							"title": "max token",
							"default": "1.0",
							"description": ""
						},
						"stream": {
							"type": "boolean",
							"title": "stream",
							"default": "false",
							"description": "Boolean to indicate if the model should stream the answer as it is being constructed"
						}
					}
				},
				"type": "string"
			}
		}
	],
	"graphs": {
		"fn-3": {
			"edges": [
				{
					"from": "fn-3-input",
					"to": "fn-3-run",
					"out": "*"
				},
				{
					"from": "fn-3-run",
					"to": "fn-3-output",
					"out": "*"
				}
			],
			"nodes": [
				{
					"id": "fn-3-input",
					"type": "input",
					"configuration": {}
				},
				{
					"id": "fn-3-run",
					"type": "runJavascript",
					"configuration": {
						"code": "function fn_3(input) {const{systemRole,userRole,systemContext,userContext,temperature,max_tokens,stream}=input;const context1={role:systemRole,content:systemContext};const context2={role:userRole,content:userContext};const payload={messages:[context1,context2],temperature,max_tokens,stream};console.log(\"payload\",payload);return{payload}}",
						"name": "fn_3",
						"raw": true
					}
				},
				{
					"id": "fn-3-output",
					"type": "output",
					"configuration": {}
				}
			]
		}
	}
}